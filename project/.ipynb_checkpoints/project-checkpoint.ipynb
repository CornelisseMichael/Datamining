{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Michael Cornelisse\"\n",
    "STUDENT_NUMBER = \"s1059020\"\n",
    "COLLABORATOR_NAME = \"Nienke Helmers\"\n",
    "COLLABORATOR_STUDENT_NUMBER = \"s1016904\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cancer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cancer\n",
       "patient        \n",
       "1             0\n",
       "2             0\n",
       "3             0\n",
       "4             0\n",
       "5             0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "#actual_data = pd.read_csv('data/archive/actual.csv')\n",
    "patient_data = pd.read_csv('data/archive/actual.csv', index_col = 'patient')\n",
    "train_data = pd.read_csv('data/archive/data_set_ALL_AML_train.csv')\n",
    "test_data = pd.read_csv('data/archive/data_set_ALL_AML_independent.csv')\n",
    "\n",
    "# Drop the call collumns from both data sets\n",
    "call_cols_train = [col for col in train_data.columns if 'call' in col]\n",
    "train_data = train_data.drop(call_cols_train, axis = 1)\n",
    "\n",
    "call_cols_test = [col for col in test_data.columns if 'call' in col]\n",
    "test_data = test_data.drop(call_cols_test, axis = 1)\n",
    "\n",
    "# Drop \"Gene Description\" and \"Gene Accession Number\"\n",
    "cols_to_drop = ['Gene Description', 'Gene Accession Number']\n",
    "train_data = train_data.drop(cols_to_drop, axis = 1)\n",
    "test_data = test_data.drop(cols_to_drop, axis = 1)\n",
    "\n",
    "# Transpose both data_sets\n",
    "train_data = train_data.T\n",
    "test_data = test_data.T\n",
    "\n",
    "# now clean the patient_data\n",
    "patient_data = patient_data.replace({'ALL': 0, 'AML': 1})\n",
    "patient_train = patient_data[patient_data.index <= 38]\n",
    "patient_test = patient_data[patient_data.index > 38]\n",
    "\n",
    "patient_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://medium.com/@snk.nitin/decision-trees-random-forests-and-pca-e676e4c142c6\n",
    "    \n",
    "In contrast, a white box model allows you to see exactly what goes on at each step, enabling you to perform the task manually if it suits you. \n",
    "Decision trees make very few assumptions about the training data and if left unconstrained they would adapt themselves to the data most likely overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "\n",
    "# Standarditization -> not necessary because decision trees are based on comparison of values and \n",
    "# scaling has no effect on the final outcome: -2947 is smaller than 2947 an the same is true for -2 and 2\n",
    "\n",
    "# PCA -> probably better for fast result, but loose explainability, as reasoned below\n",
    "'''https://www.researchgate.net/post/the_effect_PCA_in_decision_tree \n",
    "A short possible answer:\n",
    "You will no longer be able to give a nice interpretation to the decision tree, which is one of the main reason why people use them... \n",
    "On the other side, PCA might give a better representation of the data, which might (but also might not, it depends on the data itself) \n",
    "benefit any classifier (not only decision trees).\n",
    "\n",
    "A longer answer:\n",
    "As others have pointed out, PCA is a technique that projects your original data into a new coordinate space. For example, \n",
    "let's assume that you have a dataset with three features x1, x2 and x3. Then, you can find a new set of coordinates z1, z2, z3 \n",
    "(the PCA space) and project your data into this new coordinate system.\n",
    "\n",
    "With PCA, every of these new coordinates (or principal components) are a linear combination of the original features. For example:\n",
    "z1 = a*x1 + b*x2 + c*x3\n",
    "z2= d*x1 + e*x2 + f*x3\n",
    "z3 = h*x1 + h*x2 + i*x3\n",
    "\n",
    "The objective of PCA is find the coefficients a, b, c, ... such that z1 retains the maximum variance, z2 the second highest, and so on.\n",
    "When using all the principal components, you have exactly the same information, just represented in a different way. This might, or might not, be useful \n",
    "for your classifier. It depends on the data itself. If you decide to use a lower number of principal components, you will be reducing the dimensionality of \n",
    "your data. In the previous example, you might use only z1 and z2 only, reducing your data from 3, to only 2 features. Depending on your data, this might \n",
    "help classifiers to reduce the chance of overfitting, mainly if the number of instances in your dataset is relatively small. Note that this would apply to all the \n",
    "classifiers, not only decision trees. Again, this might, or might not improve the performance of your classifier, it depends on the properties of the data.\n",
    "\n",
    "An important point with respect to decision trees: People like to use them because they are easy to explain. You can analyze the tree and check how it reached \n",
    "its final decision. If you apply PCA, then the decision tree will split the data based on z1, z2, z3. Since z1, z2 and z3 are now abstract features (they are a \n",
    "linear combination of the original ones) you can no longer give a nice explanation of what is doing.\n",
    "\n",
    "Putting a concrete example, imagine that your features are age, height, and weight. It is easy to understand (and to explain) If the age is > 30 go \n",
    "left, otherwise go right. If you apply PCA you would be saying is 0.3*age + 1.5*height - 0.4*weight is < 13.5 go left, otherwise go right. As \n",
    "you can see, you have lost all the 'explainability' of the decision tree, which is one of its main characteristics.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Decision Tree\n",
    "# additional link for DecissionThreeClasifier: \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "#one decision tree\n",
    "'''\n",
    "clf = tree.DecisionTreeClassifier(criterion='gini', min_samples_split=100)\n",
    "clf = clf.fit(X, y)\n",
    "plt.figure(figsize=(30, 30))\n",
    "tree.plot_tree(clf, feature_names = attribute_names, class_names = class_names)\n",
    "plt.show()\n",
    "\n",
    "all_predictions = clf.predict(X)\n",
    "print(all_predictions)\n",
    "accuracy = clf.score(X, y)\n",
    "'''\n",
    "\n",
    "#tenfold cross validation -> FIND THE BEST DEPTH\n",
    "'''\n",
    "for train, test in kf.split(X, y):\n",
    "        split_clf = tree.DecisionTreeClassifier(criterion='gini', max_depth = i)\n",
    "        split_clf = split_clf.fit(X[train], y[train])\n",
    "        \n",
    "        accuracy_train = np.append(accuracy_train, metrics.accuracy_score(y[train], split_clf.predict(X[train])))\n",
    "        accuracy_test = np.append(accuracy_test, metrics.accuracy_score(y[test], split_clf.predict(X[test]))) \n",
    "accuracy mean for train and test data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Random Forest\n",
    "'https://towardsdatascience.com/random-forest-in-python-24d0893d51c0 ''\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "\n",
    "'one of the trees in the random forest can be visualized!'\n",
    "# Import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "# Pull out one tree from the forest\n",
    "tree = rf.estimators_[5]\n",
    "# Export the image to a dot file\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "# Use dot file to create a graph\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "# Write graph to a png file\n",
    "graph.write_png('tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Gradient Boost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
